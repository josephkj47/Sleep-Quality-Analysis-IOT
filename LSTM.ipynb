{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DataSet:\n",
        "https://physionet.org/content/sleep-accel/1.0.0/\n",
        "\n",
        "  Walch, Olivia. \"Motion and heart rate from a wrist-worn wearable and labeled sleep from polysomnography\" (version 1.0.0). PhysioNet (2019). https://doi.org/10.13026/hmhs-py35.\n",
        "\n",
        "  Olivia Walch, Yitong Huang, Daniel Forger, Cathy Goldstein, Sleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device, Sleep, Volume 42, Issue 12, December 2019, zsz180, https://doi.org/10.1093/sleep/zsz180\n",
        "\n",
        "  Goldberger, A., L. Amaral, L. Glass, J. Hausdorff, P. C. Ivanov, R. Mark, J. E. Mietus, G. B. Moody, C. K. Peng, and H. E. Stanley. \"PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215â€“e220.\" (2000).\n",
        "\n"
      ],
      "metadata": {
        "id": "D8DuGYamFavu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwJEnBUwIAiw",
        "outputId": "53d7e57a-0309-4c7c-9f9f-7f7d33bd1fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import gdown\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "oEKD6odJSRSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code local working with google drive folder perms sucks\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def convert_to_csv(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Loop through the files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        # Check if the file is a text file\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Construct the full path for input and output files\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "            output_file_path = os.path.join(output_folder, filename.replace(\".txt\", \".csv\"))\n",
        "\n",
        "            # Read data from the text file with headers\n",
        "            data = pd.read_csv(input_file_path, header=None, names=['timestamp', 'heart rate'], delimiter=',')\n",
        "\n",
        "            # Save data as CSV with headers\n",
        "            data.to_csv(output_file_path, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify input and output folders\n",
        "    heart_rate_folder = ''\n",
        "    motion_folder = ''\n",
        "    labeled_sleep_folder = ''\n",
        "\n",
        "    # Specify output folders\n",
        "    output_heart_rate_folder = ''\n",
        "    output_motion_folder = ''\n",
        "    output_labeled_sleep_folder = ''\n",
        "\n",
        "    # Convert text files to CSV for each folder\n",
        "    convert_to_csv(heart_rate_folder, output_heart_rate_folder)\n",
        "    convert_to_csv(motion_folder, output_motion_folder)\n",
        "    convert_to_csv(labeled_sleep_folder, output_labeled_sleep_folder)\n",
        "\n",
        "    print(\"Conversion completed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4s_oc32fAyzf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "92c4510b-0b4a-4b01-86a9-88c6e684c27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-68a352c03b29>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Convert text files to CSV for each folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mconvert_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheart_rate_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_heart_rate_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mconvert_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_motion_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mconvert_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_sleep_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_labeled_sleep_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-68a352c03b29>\u001b[0m in \u001b[0;36mconvert_to_csv\u001b[0;34m(input_folder, output_folder)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Create the output folder if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop through the files in the input folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code local working with google drive folder perms sucks\n",
        "# Make the same format as labels, still need some manual cut at the end but that's fine\n",
        "def preprocess_data(input_file, output_file, data_type, interval=30):\n",
        "    # Load data\n",
        "    data = pd.read_csv(input_file)\n",
        "\n",
        "    # Remove rows with negative time values\n",
        "    data = data[data['time'] >= 0]\n",
        "\n",
        "    # Convert 'time' column to datetime\n",
        "    data['time'] = pd.to_datetime(data['time'], unit='s')\n",
        "\n",
        "    # Remove duplicates in the 'time' column\n",
        "    data = data.drop_duplicates('time')\n",
        "\n",
        "    # Set 'time' column as the index\n",
        "    data.set_index('time', inplace=True)\n",
        "\n",
        "    # Resample data to have a regular time interval (interval seconds)\n",
        "    data_resampled = data.resample(f'{interval}S').mean()\n",
        "\n",
        "    # Fill missing values with the previous available value\n",
        "    data_resampled = data_resampled.fillna(method='pad')\n",
        "\n",
        "    # Reset index to get 'time' back as a column\n",
        "    data_resampled = data_resampled.reset_index()\n",
        "\n",
        "    # Convert 'time' to integer values starting from 0\n",
        "    data_resampled['time'] = (data_resampled['time'] - data_resampled['time'].min()).dt.seconds\n",
        "\n",
        "    # Save the preprocessed data\n",
        "    data_resampled.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"{data_type} data preprocessing completed for {input_file}.\")\n",
        "\n",
        "def preprocess_folder(input_folder, output_folder, data_type):\n",
        "    # Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Loop through each CSV file in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            input_file = os.path.join(input_folder, filename)\n",
        "            output_file = os.path.join(output_folder, f\"preprocessed_{filename}\")\n",
        "\n",
        "            # Preprocess each CSV file\n",
        "            preprocess_data(input_file, output_file, data_type, interval=30)\n",
        "\n",
        "# Specify input and output folders\n",
        "heart_rate_input_folder = './converted_data/heart_rate'\n",
        "motion_input_folder = './converted_data/motion'\n",
        "labels_input_file = './converted_data/labels.csv'\n",
        "\n",
        "preprocessed_heart_rate_output_folder = './preprocessed_data/heart_rate'\n",
        "preprocessed_motion_output_folder = './preprocessed_data/motion'\n",
        "\n",
        "\n",
        "# Preprocess heart rate data\n",
        "preprocess_folder(heart_rate_input_folder, preprocessed_heart_rate_output_folder, 'Heart Rate')\n",
        "\n",
        "# Preprocess motion data\n",
        "preprocess_folder(motion_input_folder, preprocessed_motion_output_folder, 'Motion')\n",
        "\n",
        "\n",
        "print(\"All preprocessing completed.\")"
      ],
      "metadata": {
        "id": "mgPDrnEadrCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine based on id\n",
        "# Specify the folder paths\n",
        "heart_rate_folder = './preprocessed_data/heart_rate'\n",
        "motion_folder = './preprocessed_data/motion'\n",
        "labels_folder = './preprocessed_data/labels'\n",
        "\n",
        "# Output folder for combined CSV files\n",
        "combined_folder = './combined_data'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(combined_folder):\n",
        "    os.makedirs(combined_folder)\n",
        "\n",
        "# Loop through the files in the heart rate folder\n",
        "for filename in os.listdir(heart_rate_folder):\n",
        "    if filename.endswith('.csv'):\n",
        "        # Extract subject ID from the filename\n",
        "        subject_id = filename.split('_')[0]\n",
        "\n",
        "        # Load heart rate data\n",
        "        heart_rate_data = pd.read_csv(os.path.join(heart_rate_folder, filename))\n",
        "\n",
        "        # Load motion data\n",
        "        motion_filename = f'{subject_id}_acceleration.csv'\n",
        "        motion_data = pd.read_csv(os.path.join(motion_folder, motion_filename))\n",
        "\n",
        "        # Load labels data\n",
        "        labels_filename = f'{subject_id}_labeled_sleep.csv'\n",
        "        labels_data = pd.read_csv(os.path.join(labels_folder, labels_filename))\n",
        "\n",
        "        # Combine the data based on a common column, e.g., 'time'\n",
        "        combined_data = pd.merge(heart_rate_data, motion_data, on='time', how='outer')\n",
        "        combined_data = pd.merge(combined_data, labels_data, on='time', how='outer')\n",
        "\n",
        "        # Save the combined data to a new CSV file\n",
        "        combined_filename = f'{subject_id}_combined.csv'\n",
        "        combined_filepath = os.path.join(combined_folder, combined_filename)\n",
        "        combined_data.to_csv(combined_filepath, index=False)\n",
        "\n",
        "print(\"Combining completed.\")"
      ],
      "metadata": {
        "id": "fbXjk2BVds6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "# LSTM Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "# Load the final combined CSV file\n",
        "final_combined_file = './final_combined.csv'\n",
        "data = pd.read_csv(final_combined_file)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Treat -1 as 0\n",
        "data['sleep_motion'] = data['sleep_motion'].replace(-1, 0)\n",
        "\n",
        "# Extract features and labels\n",
        "features = data.drop(['sleep_motion'], axis=1)  # Drop the label column\n",
        "labels = data['sleep_motion']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM model (assuming your data is time series)\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(32, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(6, activation='softmax'))  # Assuming 6 sleep stages (from 0 to 5)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy[1] * 100:.2f}%\")\n",
        "\n",
        "model.save('sleep_model.h5')\n"
      ],
      "metadata": {
        "id": "J3xDO-nD5rDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}